# -*- coding: utf-8 -*-
"""cnn_spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XU3CaqZZLYJNLx92HMapMtWtVfDIGrX_
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

import pickle
import tensorflow as tf
import wordcloud

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/spam/spam.csv",encoding='latin-1')
df.head()

data = df.copy() ## make a copy of the data
data.drop(columns=["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], inplace=True)

#rename the label and text columns
data = data.rename(columns={"v1":"label", "v2":"text"})
data.head()

data.label.value_counts()

sns.countplot(data['label'])
plt.show()

data['label'] = data['label'].map( {'spam': 1, 'ham': 0} )

data_ham  = data[data['label'] == 0].copy()
data_spam = data[data['label'] == 1].copy()
print(data_ham)

def show_wordcloud(df, title):
    text = ' '.join(df['text'].astype(str).tolist())
    stopwords = set(wordcloud.STOPWORDS)
    
    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',
                    colormap='viridis', width=800, height=600).generate(text)
    
    plt.figure(figsize=(10,7), frameon=True)
    plt.imshow(fig_wordcloud)  
    plt.axis('off')
    plt.title(title, fontsize=20 )
    plt.show()

show_wordcloud(data_spam, "Spam messages")

show_wordcloud(data_ham, "Ham messages")

# helps in text preprocessing
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# helps in model building
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import EarlyStopping

# split data into train and test set
from sklearn.model_selection import train_test_split

import re
import string
from gensim.models import Word2Vec
from gensim.test.utils import get_tmpfile
X_t = [ ]
tab = [ ]
tableau = []
X = data['text'].values
for e in X:
  #" ".join("".join([" " if ch in string.punctuation else ch for ch in e]).split())
  tab.append((e.translate(str.maketrans('', '', string.punctuation))).split(' ')) 
  for a in tab:
    tableau.append(a)
model = Word2Vec(tab,window=7, size=100,min_count=1, workers=4)
fname = get_tmpfile("/content/drive/MyDrive/spam/vectors.kv")
model.wv.save(fname)
l = model.wv.most_similar(positive=['if'], topn=5)
print(l)
print(model.wv['if'])
print("cv_to_matrix model saved")

a =[]
b = []
longueur_tab = 175
from gensim.test.utils import get_tmpfile
from gensim.models import KeyedVectors
wv = KeyedVectors.load("/content/drive/MyDrive/spam/vectors.kv", mmap='r')
for word_a_traiter in tab:
	print(wv[word_a_traiter])

# prepare tokenizer
t = Tokenizer()
t.fit_on_texts(X_train)

import re
import string
from glob import glob
import os
from os.path import basename
from pathlib import Path
from os import listdir
from os.path import isfile, join
from gensim.models import Word2Vec
from gensim.test.utils import get_tmpfile

# integer encode the documents
encoded_train = t.texts_to_sequences(X_train)
encoded_test = t.texts_to_sequences(X_test)
print(encoded_train[0:3])

# pad documents to a max length of 8words
max_length = 8
padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')
padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')
for e in padded_train:
  print(e)

vocab_size = len(t.word_index) + 1

# define the model
model = Sequential()
model.add(Embedding(vocab_size, 24, input_length=max_length))
model.add(Flatten())
model.add(Dense(500, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the model
model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# summarize the model
print(model.summary())

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)

# fit the model
model.fit(x=padded_train,
         y=y_train,
         epochs=50,
         validation_data=(padded_test, y_test), verbose=1,
         callbacks=[early_stop]
         )

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def c_report(y_true, y_pred):
   print("Classification Report")
   print(classification_report(y_true, y_pred))
   acc_sc = accuracy_score(y_true, y_pred)
   print("Accuracy : "+ str(acc_sc))
   return acc_sc

def plot_confusion_matrix(y_true, y_pred):
   mtx = confusion_matrix(y_true, y_pred)
   sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5, 
               cmap="Blues", cbar=False)
   plt.ylabel('True label')
   plt.xlabel('Predicted label')

preds = (model.predict(padded_test) > 0.5).astype("int32")

c_report(y_test, preds)

plot_confusion_matrix(y_test, preds)

model.save("/content/drive/MyDrive/spam/spam_model")

with open('/content/drive/MyDrive/spam/spam_model/assets/tokenizer.pkl', 'wb') as output:
   pickle.dump(t, output, pickle.HIGHEST_PROTOCOL)

s_model = tf.keras.models.load_model("spam_model")
with open('/content/drive/MyDrive/spam/spam_model/assets/tokenizer.pkl', 'rb') as input:
    tokenizer = pickle.load(input)

sms = ["We know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB "]
sms_proc = tokenizer.texts_to_sequences(sms)
sms_proc = pad_sequences(sms_proc, maxlen=max_length, padding='post')
pred = (model.predict(sms_proc) > 0.5).astype("int32").item()
print(pred)